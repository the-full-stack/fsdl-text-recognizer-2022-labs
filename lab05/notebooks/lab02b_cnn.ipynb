{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 02b: Training a CNN on Synthetic Handwriting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- Fundamental principles for building neural networks with convolutional components\n",
    "- How to use Lightning's training framework via a CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 2\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZN4bGgsgWc_"
   },
   "source": [
    "# Why convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9HoYWZKtTE_"
   },
   "source": [
    "The most basic neural networks,\n",
    "multi-layer perceptrons,\n",
    "are built by alternating\n",
    "parameterized linear transformations\n",
    "with non-linear transformations.\n",
    "\n",
    "This combination is capable of expressing\n",
    "[functions of arbitrary complexity](http://neuralnetworksanddeeplearning.com/chap4.html),\n",
    "so long as those functions\n",
    "take in fixed-size arrays and return fixed-size arrays.\n",
    "\n",
    "```python\n",
    "def any_function_you_can_imagine(x: torch.Tensor[\"A\"]) -> torch.Tensor[\"B\"]:\n",
    "    return some_mlp_that_might_be_impractically_huge(x)\n",
    "```\n",
    "\n",
    "But not all functions have that type signature.\n",
    "\n",
    "For example, we might want to identify the content of images\n",
    "that have different sizes.\n",
    "Without gross hacks,\n",
    "an MLP won't be able to solve this problem,\n",
    "even though it seems simple enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LjfV3o6tTFA"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import IPython.display as display\n",
    "\n",
    "randsize = 10 ** (random.random() * 2 + 1)\n",
    "\n",
    "Url = \"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/emnist/U.png\"\n",
    "\n",
    "# run multiple times to display the same image at different sizes\n",
    "#  the content of the image remains unambiguous\n",
    "display.Image(url=Url, width=randsize, height=randsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9j6YQRftTFB"
   },
   "source": [
    "Even worse, MLPs are too general to be efficient.\n",
    "\n",
    "Each layer applies an unstructured matrix to its inputs.\n",
    "But most of the data we might want to apply them to is highly structured,\n",
    "and taking advantage of that structure can make our models more efficient.\n",
    "\n",
    "It may seem appealing to use an unstructured model:\n",
    "it can in principle learn any function.\n",
    "But\n",
    "[most functions are monstrous outrages against common sense](https://en.wikipedia.org/wiki/Weierstrass_function#Density_of_nowhere-differentiable_functions).\n",
    "It is useful to encode some of our assumptions\n",
    "about the kinds of functions we might want to learn\n",
    "from our data into our model's architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvC_yZvmuwgJ"
   },
   "source": [
    "## Convolutions are the local, translation-equivariant linear transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhnRx_BZtTFC"
   },
   "source": [
    "One of the most common types of structure in data is \"locality\" --\n",
    "the most relevant information for understanding or predicting a pixel\n",
    "is a small number of pixels around it.\n",
    "\n",
    "Locality is a fundamental feature of the physical world,\n",
    "so it shows up in data drawn from physical observations,\n",
    "like photographs and audio recordings.\n",
    "\n",
    "Locality means most meaningful linear transformations of our input\n",
    "only have large weights in a small number of entries that are close to one another,\n",
    "rather than having equally large weights in all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSnkzV2_tTFC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "generic_linear_transform = torch.randn(8, 1)\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "local_linear_transform = torch.tensor([\n",
    "    [0, 0, 0] + [random.random(), random.random(), random.random()] + [0, 0]]).T\n",
    "print(\"local:\", local_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nCD75NwtTFD"
   },
   "source": [
    "Another type of structure commonly observed is \"translation equivariance\" --\n",
    "the top-left pixel position is not, in itself, meaningfully different\n",
    "from the bottom-right position\n",
    "or a position in the middle of the image.\n",
    "Relative relationships matter more than absolute relationships.\n",
    "\n",
    "Translation equivariance arises in images because there is generally no privileged\n",
    "vantage point for taking the image.\n",
    "We could just as easily have taken the image while standing a few feet to the left or right,\n",
    "and all of its contents would shift along with our change in perspective.\n",
    "\n",
    "Translation equivariance means that a linear transformation that is meaningful at one position\n",
    "in our input is likely to be meaningful at all other points.\n",
    "We can learn something about a linear transformation from a datapoint where it is useful\n",
    "in the bottom-left and then apply it to another datapoint where it's useful in the top-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srvI7JFAtTFE"
   },
   "outputs": [],
   "source": [
    "generic_linear_transform = torch.arange(8)[:, None]\n",
    "print(\"generic:\", generic_linear_transform, sep=\"\\n\")\n",
    "\n",
    "equivariant_linear_transform = torch.stack([torch.roll(generic_linear_transform[:, 0], ii) for ii in range(8)], dim=1)\n",
    "print(\"translation invariant:\", equivariant_linear_transform, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF576NCvtTFE"
   },
   "source": [
    "A linear transformation that is translation equivariant\n",
    "[is called a _convolution_](https://en.wikipedia.org/wiki/Convolution#Translational_equivariance).\n",
    "\n",
    "If the weights of that linear transformation are mostly zero\n",
    "except for a few that are close to one another,\n",
    "that convolution is said to have a _kernel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tp4tBgWtTFF"
   },
   "outputs": [],
   "source": [
    "# the equivalent of torch.nn.Linear, but for a 1-dimensional convolution\n",
    "conv_layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "conv_layer.weight  # aka kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deXA_xS6tTFF"
   },
   "source": [
    "Instead of using normal matrix multiplication to apply the kernel to the input,\n",
    "we repeatedly apply that kernel over and over again,\n",
    "\"sliding\" it over the input to produce an output.\n",
    "\n",
    "Every convolution kernel has an equivalent matrix form,\n",
    "which can be matrix multiplied with the input to create the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFoSsa5DtTFF"
   },
   "outputs": [],
   "source": [
    "conv_kernel_as_vector = torch.hstack([conv_layer.weight[0][0], torch.zeros(5)])\n",
    "conv_layer_as_matrix = torch.stack([torch.roll(conv_kernel_as_vector, ii) for ii in range(8)], dim=0)\n",
    "print(\"convolution matrix:\", conv_layer_as_matrix, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJyRtf9NtTFG"
   },
   "source": [
    "> <small> Under the hood, the actual operation that implements the application of a convolutional kernel\n",
    "need not look like either of these\n",
    "(common approaches include\n",
    "[Winograd-type algorithms](https://arxiv.org/abs/1509.09308)\n",
    "and [Fast Fourier Transform-based algorithms](https://arxiv.org/abs/1312.5851)).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xytivdcItTFG"
   },
   "source": [
    "Though they may seem somewhat arbitrary and technical,\n",
    "convolutions are actually a deep and fundamental piece of mathematics and computer science.\n",
    "Fundamental as in\n",
    "[closely related to the multiplication algorithm we learn as children](https://charlesfrye.github.io/math/2019/02/20/multiplication-convoluted-part-one.html)\n",
    "and deep as in\n",
    "[closely related to the Fourier transform](https://math.stackexchange.com/questions/918345/fourier-transform-as-diagonalization-of-convolution).\n",
    "Generalized convolutions can show up \n",
    "wherever there is some kind of \"sum\" over some kind of \"paths\",\n",
    "as is common in dynamic programming.\n",
    "\n",
    "See Chris Olah's blog series\n",
    "([1](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/),\n",
    "[2](https://colah.github.io/posts/2014-07-Understanding-Convolutions/),\n",
    "[3](https://colah.github.io/posts/2014-12-Groups-Convolution/))\n",
    "for a friendly introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCJTwCWYzRee"
   },
   "source": [
    "## We apply two-dimensional convolutions to images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8RKOPAIx0O2"
   },
   "source": [
    "In building our text recognizer,\n",
    "we're working with images.\n",
    "Images have two dimensions of translation equivariance:\n",
    "left/right and up/down.\n",
    "So we use two-dimensional convolutions,\n",
    "instantiated in `torch.nn` as `nn.Conv2d` layers.\n",
    "Note that convolutional neural networks for images\n",
    "are so popular that when the term \"convolution\"\n",
    "is used without qualifier in a neural network context,\n",
    "it can be taken to mean two-dimensional convolutions.\n",
    "\n",
    "Where `Linear` layers took in batches of vectors of a fixed size\n",
    "and returned batches of vectors of a fixed size,\n",
    "`Conv2d` layers take in batches of two-dimensional _stacked feature maps_\n",
    "and return batches of two-dimensional stacked feature maps.\n",
    "\n",
    "A pseudocode type signature based on\n",
    "[`torchtyping`](https://github.com/patrick-kidger/torchtyping)\n",
    "might look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJvMdHL7w_lu"
   },
   "source": [
    "```python\n",
    "StackedFeatureMapIn = torch.Tensor[\"batch\", \"in_channels\", \"in_height\", \"in_width\"]\n",
    "StackedFeatureMapOut = torch.Tensor[\"batch\", \"out_channels\", \"out_height\", \"out_width\"]\n",
    "def same_convolution_2d(x: StackedFeatureMapIn) -> StackedFeatureMapOut:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSMC8Fw3zPSz"
   },
   "source": [
    "Here, \"map\" is meant to evoke space:\n",
    "our feature maps tell us where\n",
    "features are spatially located.\n",
    "\n",
    "An RGB image is a stacked feature map.\n",
    "It is composed of three feature maps.\n",
    "The first tells us where the \"red\" feature is present,\n",
    "the second \"green\", the third \"blue\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIXT-mym3ljt"
   },
   "outputs": [],
   "source": [
    "display.Image(\n",
    "    url=\"https://upload.wikimedia.org/wikipedia/commons/5/56/RGB_channels_separation.png?20110219015028\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WfCcO5xJ-hG"
   },
   "source": [
    "When we apply a convolutional layer to a stacked feature map with some number of channels,\n",
    "we get back a stacked feature map with some number of channels.\n",
    "\n",
    "This output is also a stack of feature maps,\n",
    "and so it is a perfectly acceptable\n",
    "input to another convolutional layer.\n",
    "That means we can compose convolutional layers together,\n",
    "just as we composed generic linear layers together.\n",
    "We again weave non-linear functions in between our linear convolutions,\n",
    "creating a _convolutional neural network_, or CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R18TsGubJ_my"
   },
   "source": [
    "## Convolutional neural networks build up visual understanding layer by layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV03KmYBz2QM"
   },
   "source": [
    "What is the equivalent of the labels, red/green/blue,\n",
    "for the channels in these feature maps?\n",
    "What does a high activation in some position in channel 32\n",
    "of the fifteenth layer of my network tell me?\n",
    "\n",
    "There is no guaranteed way to automatically determine the answer,\n",
    "nor is there a guarantee that the result is human-interpetable.\n",
    "OpenAI's Clarity team spent several years \"reverse engineering\"\n",
    "state-of-the-art convolutiuonal neural networks trained on photographs\n",
    "and found that many of these channels are \n",
    "[directly interpretable](https://distill.pub/2018/building-blocks/).\n",
    "\n",
    "For example, they found that if they pass an image through\n",
    "[GoogLeNet](https://doi.org/10.1109/cvpr.2015.7298594),\n",
    "aka InceptionV1,\n",
    "the winner of the\n",
    "[2014 ImageNet Very Large Scale Visual Recognition Challenge](https://www.image-net.org/challenges/LSVRC/2014/),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64KJR70q6dCh"
   },
   "outputs": [],
   "source": [
    "# a sample image\n",
    "display.Image(url=\"https://distill.pub/2018/building-blocks/examples/input_images/dog_cat.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ7CvvG78CZ5"
   },
   "source": [
    "the features become increasingly complex,\n",
    "with channels in early layers (left)\n",
    "acting as maps for simple things like \"high frequency power\" or \"45 degree black-white edge\"\n",
    "and channels in later layers (to right)\n",
    "acting as feature maps for increasingly abstract concepts,\n",
    "like \"circle\" and eventually \"floppy round ear\" or \"pointy ear\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w5_RR8d9jEY"
   },
   "outputs": [],
   "source": [
    "# from https://distill.pub/2018/building-blocks/\n",
    "display.Image(url=\"https://fsdl-public-assets.s3.us-west-2.amazonaws.com/distill-feature-attrib.png\", width=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLiqEwMY_Co0"
   },
   "source": [
    "> <small> The small square images depict a heuristic estimate\n",
    "of what the entire collection of feature maps\n",
    "at a given layer represent (layer IDs at bottom).\n",
    "They are arranged in a spatial grid and their sizes represent\n",
    "the total magnitude of the layer's activations at that position.\n",
    "For details and interactivity, see\n",
    "[the original Distill article](https://distill.pub/2018/building-blocks/).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl8XlEsaA54W"
   },
   "source": [
    "In the\n",
    "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
    "blogpost series,\n",
    "the Open AI Clarity team\n",
    "combines careful examination of weights\n",
    "with direct experimentation\n",
    "to build an understanding of how these higher-level features\n",
    "are constructed in GoogLeNet.\n",
    "\n",
    "For example,\n",
    "they are able to provide reasonable interpretations for\n",
    "[almost every channel in the first five layers](https://distill.pub/2020/circuits/early-vision/).\n",
    "\n",
    "The cell below will pull down their \"weight explorer\" \n",
    "and embed it in this notebook.\n",
    "By default, it starts on\n",
    "[the 52nd channel in the `conv2d1` layer](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d1_52.html),\n",
    "which constructs a large, phase-invariant\n",
    "[Gabor filter](https://en.wikipedia.org/wiki/Gabor_filter)\n",
    "from smaller, phase-sensitive filters.\n",
    "It is in turn used to construct\n",
    "[curve](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_180.html)\n",
    "and\n",
    "[texture](https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/conv2d2_114.html)\n",
    "detectors --\n",
    "click on any image to navigate to the weight explorer page\n",
    "for that channel\n",
    "or change the `layer` and `idx`\n",
    "arguments.\n",
    "For additional context,\n",
    "check out the\n",
    "[Early Vision in InceptionV1 blogpost](https://distill.pub/2020/circuits/early-vision/).\n",
    "\n",
    "Click the \"View this neuron in the OpenAI Microscope\" link\n",
    "for an even richer interactive view,\n",
    "including activations on sample images\n",
    "([example](https://microscope.openai.com/models/inceptionv1/conv2d1_0/52)).\n",
    "\n",
    "The\n",
    "[Circuits Thread](https://distill.pub/2020/circuits/)\n",
    "which this explorer accompanies\n",
    "is chock-full of empirical observations, theoretical speculation, and nuggets of wisdom\n",
    "that are invaluable for developing intuition about both\n",
    "convolutional networks in particular and visual perception in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4-hkYjdB-qQ"
   },
   "outputs": [],
   "source": [
    "layers = [\"conv2d0\", \"conv2d1\", \"conv2d2\", \"mixed3a\", \"mixed3b\"]\n",
    "layer = layers[1]\n",
    "idx = 52\n",
    "\n",
    "weight_explorer = display.IFrame(\n",
    "    src=f\"https://storage.googleapis.com/distill-circuits/inceptionv1-weight-explorer/{layer}_{idx}.html\", width=1024, height=720)\n",
    "weight_explorer.iframe = 'style=\"background: #FFF\";\\n><'.join(weight_explorer.iframe.split(\"><\"))  # inject background color\n",
    "weight_explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ6_PCmVtTFH"
   },
   "source": [
    "# Applying convolutions to handwritten characters: `CNN`s on `EMNIST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N--VkRtR5Yr-"
   },
   "source": [
    "If we load up the `CNN` class from `text_recognizer.models`,\n",
    "we'll see that a `data_config` is required to instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3MA--zytTFH"
   },
   "outputs": [],
   "source": [
    "import text_recognizer.models\n",
    "\n",
    "\n",
    "text_recognizer.models.CNN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yCP46PO6XDg"
   },
   "source": [
    "So before we can make our convolutional network and train it,\n",
    "we'll need to get a hold of some data.\n",
    "This isn't a general constraint by the way --\n",
    "it's an implementation detail of the `text_recognizer` library.\n",
    "But datasets and models are generally coupled,\n",
    "so it's common for them to share configuration information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Z42K-jjtTFH"
   },
   "source": [
    "## The `EMNIST` Handwritten Character Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiifKuu4tTFH"
   },
   "source": [
    "We could just use `MNIST` here,\n",
    "as we did in\n",
    "[the first lab](https://fsdl.me/lab01-colab).\n",
    "\n",
    "But we're aiming to eventually build a handwritten text recognition system,\n",
    "which means we need to handle letters and punctuation,\n",
    "not just numbers.\n",
    "\n",
    "So we instead use _EMNIST_,\n",
    "or [Extended MNIST](https://paperswithcode.com/paper/emnist-an-extension-of-mnist-to-handwritten),\n",
    "which includes letters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ePZW1Tfa00K"
   },
   "outputs": [],
   "source": [
    "import text_recognizer.data\n",
    "\n",
    "\n",
    "emnist = text_recognizer.data.EMNIST()  # configure\n",
    "print(emnist.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_yjBYhla6qp"
   },
   "source": [
    "We've built a PyTorch Lightning `DataModule`\n",
    "to encapsulate all the code needed to get this dataset ready to go:\n",
    "downloading to disk,\n",
    "[reformatting to make loading faster](https://www.h5py.org/),\n",
    "and splitting into training, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty2vakBBtTFI"
   },
   "outputs": [],
   "source": [
    "emnist.prepare_data()  # download, save to disk\n",
    "emnist.setup()  # create torch.utils.data.Datasets, do train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h9bAXcu8l5J"
   },
   "source": [
    "A brief aside: you might be wondering where this data goes.\n",
    "Datasets are saved to disk inside the repo folder,\n",
    "but not tracked in version control.\n",
    "`git` works well for versioning source code\n",
    "and other text files, but it's a poor fit for large binary data.\n",
    "We only track and version metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5cwDCM88SnU"
   },
   "outputs": [],
   "source": [
    "!echo {emnist.data_dirname()}\n",
    "!ls {emnist.data_dirname()}\n",
    "!ls {emnist.data_dirname() / \"raw\" / \"emnist\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdsIBL9MtTFI"
   },
   "source": [
    "This class comes with a pretty printing method\n",
    "for quick examination of some of that metadata and basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cyw66d6GtTFI"
   },
   "outputs": [],
   "source": [
    "emnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT0burlOLgoH"
   },
   "source": [
    "\n",
    "> <small> You can add pretty printing to your own Python classes by writing\n",
    "`__str__` or `__repr__` methods for them.\n",
    "The former is generally expected to be human-readable,\n",
    "while the latter is generally expected to be machine-readable;\n",
    "we've broken with that custom here and used `__repr__`. </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJF3G5idtTFI"
   },
   "source": [
    "Because we've run `.prepare_data` and `.setup`,\n",
    "we can expect that this `DataModule` is ready to provide a `DataLoader`\n",
    "if we invoke the right method --\n",
    "sticking to the PyTorch Lightning API brings these kinds of convenient guarantees\n",
    "even when we're not using the `Trainer` class itself,\n",
    "[as described in Lab 2a](https://fsdl.me/lab02a-colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJghcZkWtTFI"
   },
   "outputs": [],
   "source": [
    "xs, ys = next(iter(emnist.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40FWjMT-tTFJ"
   },
   "source": [
    "Run the cell below to inspect random elements of this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hywyEI_tTFJ"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "idx = random.randint(0, len(xs) - 1)\n",
    "\n",
    "print(emnist.mapping[ys[idx]])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdg_wYWntTFJ"
   },
   "source": [
    "## Putting convolutions in a `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGuSx_zvtTFJ"
   },
   "source": [
    "Because we have the data,\n",
    "we now have a `data_config`\n",
    "and can instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxLf7-5jtTFJ"
   },
   "outputs": [],
   "source": [
    "data_config = emnist.config()\n",
    "\n",
    "cnn = text_recognizer.models.CNN(data_config)\n",
    "cnn  # reveals the nn.Modules attached to our nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkeJNVnIMVzJ"
   },
   "source": [
    "We can run this network on our inputs,\n",
    "but we don't expect it to produce correct outputs without training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EwujOGqMAZY"
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(xs) - 1)\n",
    "outs = cnn(xs[idx:idx+1])\n",
    "\n",
    "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3L8u0estTFJ"
   },
   "source": [
    "We can inspect the `.forward` method to see how these `nn.Module`s are used.\n",
    "\n",
    "> <small> Note: we encourage you to read through the code --\n",
    "either inside the notebooks, as below,\n",
    "in your favorite text editor locally, or\n",
    "[on GitHub](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022-labs).\n",
    "There's lots of useful bits of Python that we don't have time to cover explicitly in the labs.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtA0W8jvtTFJ"
   },
   "outputs": [],
   "source": [
    "cnn.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCycQ88gtTFK"
   },
   "source": [
    "We apply convolutions followed by non-linearities,\n",
    "with intermittent \"pooling\" layers that apply downsampling --\n",
    "similar to the 1989\n",
    "[LeNet](https://doi.org/10.1162%2Fneco.1989.1.4.541)\n",
    "architecture or the 2012\n",
    "[AlexNet](https://doi.org/10.1145%2F3065386)\n",
    "architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkGJCnMttTFK"
   },
   "source": [
    "The final classification is performed by an MLP.\n",
    "\n",
    "In order to get vectors to pass into that MLP,\n",
    "we first apply `torch.flatten`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZPhw7ufAKZ7"
   },
   "outputs": [],
   "source": [
    "torch.flatten(torch.Tensor([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCoCa3vCNM8j"
   },
   "source": [
    "## Design considerations for CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDLEMnPINTj7"
   },
   "source": [
    "Since the release of AlexNet,\n",
    "there has been a feverish decade of engineering and innovation in CNNs --\n",
    "[dilated convolutions](https://arxiv.org/abs/1511.07122),\n",
    "[residual connections](https://arxiv.org/abs/1512.03385), and\n",
    "[batch normalization](https://arxiv.org/abs/1502.03167)\n",
    "came out in 2015 alone, and\n",
    "[work continues](https://arxiv.org/abs/2201.03545) --\n",
    "so we can only scratch the surface in this course and\n",
    "[the devil is in the details](https://arxiv.org/abs/1405.3531v4).\n",
    "\n",
    "The progress of DNNs in general and CNNs in particular\n",
    "has been mostly evolutionary,\n",
    "with lots of good ideas that didn't work out\n",
    "and weird hacks that stuck around because they did.\n",
    "That can make it very hard to design a fresh architecture\n",
    "from first principles that's anywhere near as effective as existing architectures.\n",
    "You're better off tweaking and mutating an existing architecture\n",
    "than trying to design one yourself.\n",
    "\n",
    "If you're not keeping close tabs on the field,\n",
    "when your first start looking for an architecture to base your work off of\n",
    "it's best to go to trusted aggregators, like\n",
    "[Torch IMage Models](https://github.com/rwightman/pytorch-image-models),\n",
    "or `timm`, on GitHub, or\n",
    "[Papers With Code](https://paperswithcode.com),\n",
    "specifically the section for\n",
    "[computer vision](https://paperswithcode.com/methods/area/computer-vision).\n",
    "You can also take a more bottom-up approach by checking\n",
    "the leaderboards of the latest\n",
    "[Kaggle competitions on computer vision](https://www.kaggle.com/competitions?searchQuery=computer+vision).\n",
    "\n",
    "We'll briefly touch here on some of the main design considerations\n",
    "with classic CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd0OeyouDNlS"
   },
   "source": [
    "### Shapes and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w3p8QP6AnGQ"
   },
   "source": [
    "In the `.forward` pass of the `CNN`,\n",
    "we've included comments that indicate the expected shapes\n",
    "of tensors after each line that changes the shape.\n",
    "\n",
    "Tracking and correctly handling shapes is one of the bugbears\n",
    "of CNNs, especially architectures,\n",
    "like LeNet/AlexNet, that include MLP components\n",
    "that can only operate on fixed-shape tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgbM30jstTFK"
   },
   "source": [
    "[Shape arithmetic gets pretty hairy pretty fast](https://arxiv.org/abs/1603.07285)\n",
    "if you're supporting the wide variety of convolutions.\n",
    "\n",
    "The easiest way to avoid shape bugs is to keep things simple: \n",
    "choose your convolution parameters,\n",
    "like `padding` and `stride`,\n",
    "to keep the shape the same before and after\n",
    "the convolution.\n",
    "\n",
    "That's what we do, by choosing `padding=1`\n",
    "for `kernel_size=3` and `stride=1`.\n",
    "With unit strides and odd-numbered kernel size,\n",
    "the padding that keeps\n",
    "the input the same size is `kernel_size // 2`.\n",
    "\n",
    "As shapes change, so does the amount of GPU memory taken up by the tensors.\n",
    "Keeping sizes fixed within a block removes one axis of variation\n",
    "in the demands on an important resource.\n",
    "\n",
    "After applying our pooling layer,\n",
    "we can just increase the number of kernels by the right factor\n",
    "to keep total tensor size,\n",
    "and thus memory footprint, constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BCkTZGSDSBG"
   },
   "source": [
    "### Parameters, computation, and bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZbgm7wztTFK"
   },
   "source": [
    "If we review the `num`ber of `el`ements in each of the layers,\n",
    "we see that one layer has far more entries than all the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nfjPVwztTFK"
   },
   "outputs": [],
   "source": [
    "[p.numel() for p in cnn.parameters()]  # conv weight + bias, conv weight + bias, fc weight + bias, fc weight + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzIoCz1FtTFK"
   },
   "source": [
    "The biggest layer is typically\n",
    "the one in between the convolutional component\n",
    "and the MLP component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYrlUprltTFK"
   },
   "outputs": [],
   "source": [
    "biggest_layer = [p for p in cnn.parameters() if p.numel() == max(p.numel() for p in cnn.parameters())][0]\n",
    "biggest_layer.shape, cnn.fc_input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSHdvEGptTFL"
   },
   "source": [
    "This layer dominates the cost of storing the network on disk.\n",
    "That makes it a common target for\n",
    "regularization techniques like DropOut\n",
    "(as in our architecture)\n",
    "and performance optimizations like\n",
    "[pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html).\n",
    "\n",
    "Heuristically, we often associated more parameters with more computation.\n",
    "But just because that layer has the most parameters\n",
    "does not mean that most of the compute time is spent in that layer.\n",
    "\n",
    "Convolutions reuse the same parameters over and over,\n",
    "so the total number of FLOPs done by the layer can be higher\n",
    "than that done by layers with more parameters --\n",
    "much higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLisj1SptTFL"
   },
   "outputs": [],
   "source": [
    "# for the Linear layers, number of multiplications per input == nparams\n",
    "cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo2oINHRtTFL"
   },
   "outputs": [],
   "source": [
    "# for the Conv2D layers, it's more complicated\n",
    "\n",
    "def approx_conv_multiplications(kernel_shape, input_size=(64, 28, 28)):  # this is a rough and dirty approximation\n",
    "    num_kernel_elements = 1\n",
    "    for dimension in kernel_shape[-3:]:\n",
    "        num_kernel_elements *= dimension\n",
    "    num_input_channels, num_kernels = input_size[0], kernel_shape[0]\n",
    "    num_spatial_applications = ((input_size[1] - kernel_shape[-2] + 1) * (input_size[2] - kernel_shape[-1] + 1))\n",
    "    mutliplications_per_kernel = num_spatial_applications * num_kernel_elements * num_input_channels\n",
    "    return mutliplications_per_kernel * num_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwCbZU9PtTFL"
   },
   "outputs": [],
   "source": [
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdco4m9UtTFL"
   },
   "outputs": [],
   "source": [
    "# ratio of multiplications in the convolution to multiplications in the fully-connected layer is huge!\n",
    "approx_conv_multiplications(cnn.conv2.conv.weight.shape) // cnn.fc1.weight.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joVoBEtqtTFL"
   },
   "source": [
    "Depending on your compute hardware and the problem characteristics,\n",
    "either the MLP component or the convolutional component\n",
    "could become the critical bottleneck.\n",
    "\n",
    "When you're memory constrained, like when transferring a model \"over the wire\" to a browser,\n",
    "the MLP component is likely to be the bottleneck,\n",
    "whereas when you are compute-constrained, like when running a model on a low-power edge device\n",
    "or in an application with strict low-latency requirements,\n",
    "the convolutional component is likely to be the bottleneck.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGSyp67dtTFM"
   },
   "source": [
    "## Training a `CNN` on `EMNIST` with the Lightning `Trainer` and `run_experiment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYTJs7snQfX0"
   },
   "source": [
    "We have a model and we have data,\n",
    "so we could just go ahead and start training in raw PyTorch,\n",
    "[as we did in Lab 01](https://fsdl.me/lab01-colab).\n",
    "\n",
    "But as we saw in that lab,\n",
    "there are good reasons to use a framework\n",
    "to organize training and provide fixed interfaces and abstractions.\n",
    "So we're going to use PyTorch Lightning, which is\n",
    "[covered in detail in Lab 02a](https://fsdl.me/lab02a-colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZYaJ4bdMcWc"
   },
   "source": [
    "We provide a simple script that implements a command line interface\n",
    "to training with PyTorch Lightning\n",
    "using the models and datasets in this repository:\n",
    "`training/run_experiment.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52kIYhPBPLNZ"
   },
   "outputs": [],
   "source": [
    "%run training/run_experiment.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkM_HpILSyC9"
   },
   "source": [
    "The `pl.Trainer` arguments come first\n",
    "and there\n",
    "[are a lot of them](https://pytorch-lightning.readthedocs.io/en/1.6.3/common/trainer.html),\n",
    "so if we want to see what's configurable for\n",
    "our `Model` or our `LitModel`,\n",
    "we want the last few dozen lines of the help message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0dBhgogO8_A"
   },
   "outputs": [],
   "source": [
    "!python training/run_experiment.py --help --model_class CNN --data_class EMNIST  | tail -n 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCBQekrPRt90"
   },
   "source": [
    "The `run_experiment.py` file is also importable as a module,\n",
    "so that you can inspect its contents\n",
    "and play with its component functions in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPumvYatPaiS"
   },
   "outputs": [],
   "source": [
    "import training.run_experiment\n",
    "\n",
    "\n",
    "print(training.run_experiment.main.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiZ3RwW2UzJm"
   },
   "source": [
    "Let's run training!\n",
    "\n",
    "Execute the cell below to launch a training job for a CNN on EMNIST with default arguments.\n",
    "\n",
    "This will take several minutes on commodity hardware,\n",
    "so feel free to keep reading while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RSJM5I2TSeG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpus = int(torch.cuda.is_available())  # use GPUs if they're available\n",
    "\n",
    "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ayQ4ByJOnnP"
   },
   "source": [
    "The first thing you'll see are a few logger messages from Lightning,\n",
    "then some info about the hardware you have available and are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcMrZcecO1EF"
   },
   "source": [
    "Then you'll see a summary of your model,\n",
    "including module names, parameter counts,\n",
    "and information about model disk size.\n",
    "\n",
    "`torchmetrics` show up here as well,\n",
    "since they are also `nn.Module`s.\n",
    "See [Lab 02a](https://fsdl.me/lab02a-colab)\n",
    "for details.\n",
    "We're tracking accuracy on training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twGp9iWOUSfc"
   },
   "source": [
    "You may also see a quick message in the terminal\n",
    "referencing a \"validation sanity check\".\n",
    "PyTorch Lightning runs a few batches of validation data\n",
    "through the model before the first training epoch.\n",
    "This helps prevent training runs from crashing\n",
    "at the end of the first epoch,\n",
    "which is otherwise the first time validation loops are triggered\n",
    "and is sometimes hours into training,\n",
    "by crashing them quickly at the start.\n",
    "\n",
    "If you want to turn off the check,\n",
    "use `--num_sanity_val_steps=0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnKN3_MiRpE4"
   },
   "source": [
    "Then, you'll see a bar indicating\n",
    "progress through the training epoch,\n",
    "alongside metrics like throughput and loss.\n",
    "\n",
    "When the first (and only) epoch ends,\n",
    "the model is run on the validation set\n",
    "and aggregate loss and accuracy are reported to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2eMZz_HR8vV"
   },
   "source": [
    "At the end of training,\n",
    "we call `Trainer.test`\n",
    "to check performance on the test set.\n",
    "\n",
    "We typically see test accuracy around 75-80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybpLiKBKSDXI"
   },
   "source": [
    "During training, PyTorch Lightning saves _checkpoints_\n",
    "(file extension `.ckpt`)\n",
    "that can be used to restart training.\n",
    "\n",
    "The final line output by `run_experiment`\n",
    "indicates where the model with the best performance\n",
    "on the validation set has been saved.\n",
    "\n",
    "The checkpointing behavior is configured using a\n",
    "[`ModelCheckpoint` callback](https://pytorch-lightning.readthedocs.io/en/1.6.3/api/pytorch_lightning.callbacks.ModelCheckpoint.html).\n",
    "The `run_experiment` script picks sensible defaults.\n",
    "\n",
    "These checkpoints contain the model weights.\n",
    "We can use them to los the model in the notebook and play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Rqh9ZQsY8g4"
   },
   "outputs": [],
   "source": [
    "# we use a sequence of bash commands to get the latest checkpoint's filename\n",
    "#  by hand, you can just copy and paste it\n",
    "\n",
    "list_all_log_files = \"find training/logs/lightning_logs\"  # find avoids issues with \\n in filenames\n",
    "filter_to_ckpts = \"grep \\.ckpt$\"  # regex match on end of line\n",
    "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
    "take_first = \"head -n 1\"  # the first n elements, n=1\n",
    "\n",
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "latest_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QW_CxR3coV6"
   },
   "source": [
    "To rebuild the model,\n",
    "we need to consider some implementation details of the `run_experiment` script.\n",
    "\n",
    "We use the parsed command line arguments, the `args`, to build the data and model,\n",
    "then use all three to build the `LightningModule`.\n",
    "\n",
    "Any `LightningModule` can be reinstantiated from a checkpoint\n",
    "using the `load_from_checkpoint` method,\n",
    "but we'll need to recreate and pass the `args`\n",
    "in order to reload the model.\n",
    "(We'll see how this can be automated later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVWEHcgvaSqZ"
   },
   "outputs": [],
   "source": [
    "import training.util\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "# if you change around model/data args in the command above, add them here\n",
    "#  tip: define the arguments as variables, like we've done for gpus\n",
    "#       and then add those variables to this dict so you don't need to\n",
    "#       remember to update/copy+paste\n",
    "\n",
    "args = Namespace(**{\n",
    "    \"model_class\": \"CNN\",\n",
    "    \"data_class\": \"EMNIST\"})\n",
    "\n",
    "\n",
    "_, cnn = training.util.setup_data_and_model_from_args(args)\n",
    "\n",
    "reloaded_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
    "   latest_ckpt, args=args, model=cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MynyI_eUcixa"
   },
   "source": [
    "With the model reloads, we can run it on some sample data\n",
    "and see how it's doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0HCxgVwcRAA"
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(xs) - 1)\n",
    "outs = reloaded_model(xs[idx:idx+1])\n",
    "\n",
    "print(\"output:\", emnist.mapping[torch.argmax(outs)])\n",
    "wandb.Image(xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6NtaHuVdfqt"
   },
   "source": [
    "I generally see subjectively good performance --\n",
    "without seeing the labels, I tend to agree with the model's output\n",
    "more often than the accuracy would suggest,\n",
    "since some classes, like c and C or o, O, and 0,\n",
    "are essentially indistinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZzcDcxpVkki"
   },
   "source": [
    "We can continue a promising training run from the checkpoint.\n",
    "Run the cell below to train the model just trained above\n",
    "for another epoch.\n",
    "Note that the training loss starts out closs to where it ended\n",
    "in the previous run.\n",
    "\n",
    "Paired with cloud storage of checkpoints,\n",
    "this makes it possible to use\n",
    "[a cheaper type of cloud instance](https://cloud.google.com/blog/products/ai-machine-learning/reduce-the-costs-of-ml-workflows-with-preemptible-vms-and-gpus)\n",
    "that can be pre-empted by someone willing to pay more,\n",
    "which terminates your job.\n",
    "It's also helpful when using Google Colab for more serious projects --\n",
    "your training runs are no longer bound by the maximum uptime of a Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skqdikNtVnaf"
   },
   "outputs": [],
   "source": [
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "\n",
    "\n",
    "# and we can change the training hyperparameters, like batch size\n",
    "%run training/run_experiment.py --model_class CNN --data_class EMNIST --gpus {gpus} \\\n",
    "  --batch_size 64 --load_checkpoint {latest_ckpt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBdNt6Z2tTFM"
   },
   "source": [
    "# Creating lines of text from handwritten characters: `EMNISTLines`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FevtQpeDtTFM"
   },
   "source": [
    "We've got a training pipeline for our model and our data,\n",
    "and we can use that to make the loss go down\n",
    "and get better at the task.\n",
    "But the problem we're solving not obviously useful:\n",
    "the model is just learning hwo to handle\n",
    "centered, high-contrast, isolated characters.\n",
    "\n",
    "To make this work in a text recognition application,\n",
    "we would need a component to first pull out characters like that from images.\n",
    "That task is probably harder than the one we're currently learning.\n",
    "Plus, splitting into two separate components is against the ethos of deep learning,\n",
    "which operates \"end-to-end\".\n",
    "\n",
    "Let's kick the realism up one notch by building lines of text out of our characters:\n",
    "_synthesizing_ data for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7i4JhWe7ch"
   },
   "source": [
    "Synthetic data is generally useful for augmenting limited real data.\n",
    "By construction we know the labels, since we created the data.\n",
    "Often, we can track covariates,\n",
    "like lighting features or subclass membership,\n",
    "that aren't always available in our labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrQ_44TIe39m"
   },
   "source": [
    "To build fake handwriting,\n",
    "we'll combine two things:\n",
    "real handwritten letters and real text.\n",
    "\n",
    "We generate our fake text by drawing from the\n",
    "[Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
    "provided by the [`n`atural `l`anguage `t`ool`k`it](https://www.nltk.org/) library.\n",
    "\n",
    "First, we download that corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtSg7Y8Ydxpa"
   },
   "outputs": [],
   "source": [
    "from text_recognizer.data.sentence_generator import SentenceGenerator\n",
    "\n",
    "sentence_generator = SentenceGenerator()\n",
    "\n",
    "SentenceGenerator.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yal5eHk-aB4i"
   },
   "source": [
    "We can generate short snippets of text from the corpus with the `SentenceGenerator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRg_C1TYzwKX"
   },
   "outputs": [],
   "source": [
    "print(*[sentence_generator.generate(max_length=16) for _ in range(4)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGsBuMICaXnM"
   },
   "source": [
    "We use another `DataModule` to pick out the needed handwritten characters from `EMNIST`\n",
    "and glue them together into images containing the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtsGfSu6dpZ9"
   },
   "outputs": [],
   "source": [
    "emnist_lines = text_recognizer.data.EMNISTLines()  # configure\n",
    "emnist_lines.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dik_SyEdb0st"
   },
   "source": [
    "This can take several minutes when first run,\n",
    "but afterwards data is persisted to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SofIYHOUtTFM"
   },
   "outputs": [],
   "source": [
    "emnist_lines.prepare_data()  # download, save to disk\n",
    "emnist_lines.setup()  # create torch.utils.data.Datasets, do train/val split\n",
    "emnist_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axESuV1SeoM6"
   },
   "source": [
    "Again, we're using the `LightningDataModule` interface\n",
    "to organize our data prep,\n",
    "so we can now fetch a batch and take a look at some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J7f2I9ggBi-"
   },
   "outputs": [],
   "source": [
    "line_xs, line_ys = next(iter(emnist_lines.val_dataloader()))\n",
    "line_xs.shape, line_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0yHgbW2gHgP"
   },
   "outputs": [],
   "source": [
    "def read_line_labels(labels):\n",
    "    return [emnist_lines.mapping[label] for label in labels]\n",
    "\n",
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "print(\"-\".join(read_line_labels(line_ys[idx])))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xirEmNPNtTFM"
   },
   "source": [
    "The result looks\n",
    "[kind of like a ransom note](https://tvtropes.org/pmwiki/pmwiki.php/Main/CutAndPasteNote)\n",
    "and is not yet anywhere near realistic, even for single lines --\n",
    "letters don't overlap, the exact same handwritten letter is repeated\n",
    "if the character appears more than once in the snippet --\n",
    "but it's a start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRWbSzkotTFM"
   },
   "source": [
    "# Applying CNNs to handwritten text: `LineCNNSimple`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzwYBv82tTFM"
   },
   "source": [
    "The `LineCNNSimple` class builds on the `CNN` class and can be applied to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqeImjd2lF7p"
   },
   "outputs": [],
   "source": [
    "line_cnn = text_recognizer.models.LineCNNSimple(emnist_lines.config())\n",
    "line_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi6g0acoxJO4"
   },
   "source": [
    "The `nn.Module`s look much the same,\n",
    "but the way they are used is different,\n",
    "which we can see by examining the `.forward` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qg3UJhibxHfC"
   },
   "outputs": [],
   "source": [
    "line_cnn.forward??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAW7EWVlxMhd"
   },
   "source": [
    "The `CNN`, which operates on square images,\n",
    "is applied to our wide image repeatedly,\n",
    "slid over by the `W`indow `S`ize each time.\n",
    "We effectively convolve the network with the input image.\n",
    "\n",
    "Like our synthetic data, it is crude\n",
    "but it's enough to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU4J13yLisiC"
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "outs, = line_cnn(line_xs[idx:idx+1])\n",
    "preds = torch.argmax(outs, 0)\n",
    "\n",
    "print(\"-\".join(read_line_labels(preds)))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxHI4Gzndbxg"
   },
   "source": [
    "> <small> You may notice that this randomly-initialized\n",
    "network tends to predict some characters far more often than others,\n",
    "rather than predicting all characters with equal likelihood.\n",
    "This is a commonly-observed phenomenon in deep networks.\n",
    "It is connected to issues with\n",
    "[model calibration](https://arxiv.org/abs/1706.04599)\n",
    "and Bayesian uses of DNNs\n",
    "(see e.g. Figure 7 of\n",
    "[Wenzel et al. 2020](https://arxiv.org/abs/2002.02405).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSonI9KcfJrB"
   },
   "source": [
    "Let's launch a training run with the default parameters.\n",
    "\n",
    "This cell should run in just a few minutes on typical hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsbJdeRiwSVA"
   },
   "outputs": [],
   "source": [
    "%run training/run_experiment.py --model_class LineCNNSimple --data_class EMNISTLines \\\n",
    "  --batch_size 32 --gpus {gpus} --max_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9e5nTplfoXG"
   },
   "source": [
    "You should see a test accuracy in the 65-70% range.\n",
    "\n",
    "That seems pretty good,\n",
    "especially for a simple model trained in a minute.\n",
    "\n",
    "Let's reload the model and run it on some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NuXazAvw9NA"
   },
   "outputs": [],
   "source": [
    "# if you change around model/data args in the command above, add them here\n",
    "#  tip: define the arguments as variables, like we've done for gpus\n",
    "#       and then add those variables to this dict so you don't need to\n",
    "#       remember to update/copy+paste\n",
    "\n",
    "args = Namespace(**{\n",
    "    \"model_class\": \"LineCNNSimple\",\n",
    "    \"data_class\": \"EMNISTLines\"})\n",
    "\n",
    "\n",
    "_, line_cnn = training.util.setup_data_and_model_from_args(args)\n",
    "\n",
    "latest_ckpt, = ! {list_all_log_files} | {filter_to_ckpts} | {sort_version_descending} | {take_first}\n",
    "print(latest_ckpt)\n",
    "\n",
    "reloaded_lines_model = text_recognizer.lit_models.BaseLitModel.load_from_checkpoint(\n",
    "   latest_ckpt, args=args, model=line_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8ziVROkxkGC"
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(line_xs) - 1)\n",
    "\n",
    "outs, = reloaded_lines_model(line_xs[idx:idx+1])\n",
    "preds = torch.argmax(outs, 0)\n",
    "\n",
    "print(\"-\".join(read_line_labels(preds)))\n",
    "wandb.Image(line_xs[idx]).image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9bQCHtYgA0S"
   },
   "source": [
    "In general,\n",
    "we see predictions that have very low subjective quality:\n",
    "it seems like most of the letters are wrong\n",
    "and the model often prefers to predict the most common letters\n",
    "in the dataset, like `e`.\n",
    "\n",
    "Notice, however, that many of the\n",
    "characters in a given line are padding characters, `<P>`.\n",
    "\n",
    "A model that always predicts `<P>` can achieve around 50% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE-T7zgDgo7-"
   },
   "outputs": [],
   "source": [
    "padding_token = emnist_lines.emnist.inverse_mapping[\"<P>\"]\n",
    "torch.sum(line_ys == padding_token) / line_ys.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGHWmOyVh5rV"
   },
   "source": [
    "There are ways to adjust your classification metrics to\n",
    "[handle this particular issue](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall).\n",
    "In general it's good to find a metric\n",
    "that has baseline performance at 0 and perfect performance at 1,\n",
    "so that numbers are clearly interpretable.\n",
    "\n",
    "But it's an important reminder to actually look\n",
    "at your model's behavior from time to time.\n",
    "Metrics are single numbers,\n",
    "so they by necessity throw away a ton of information\n",
    "about your model's behavior,\n",
    "some of which is deeply relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p--KWZ9YJWQ"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srQnoOK8YLDv"
   },
   "source": [
    "###  Research a `pl.Trainer` argument and try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j652MtkYR8n"
   },
   "source": [
    "The Lightning `Trainer` class is highly configurable\n",
    "and has accumulated a number of features as Lightning has matured.\n",
    "\n",
    "Check out the documentation for this class\n",
    "and pick an argument to try out with `training/run_experiment.py`.\n",
    "Look for edge cases in its behavior,\n",
    "especially when combined with other arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UWNicq_jS7k"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "pl_version = pl.__version__\n",
    "\n",
    "print(\"pl.Trainer guide URL:\", f\"https://pytorch-lightning.readthedocs.io/en/{pl_version}/common/trainer.html\")\n",
    "print(\"pl.Trainer reference docs URL:\", f\"https://pytorch-lightning.readthedocs.io/en/{pl_version}/api/pytorch_lightning.trainer.trainer.Trainer.html\")\n",
    "\n",
    "pl.Trainer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14AOfjqqYOoT"
   },
   "outputs": [],
   "source": [
    "%run training/run_experiment.py --help"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab02b_cnn.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f056848cf5d2396a4970b625f23716aa539c2ff5334414c1b5d98d7daae66f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
